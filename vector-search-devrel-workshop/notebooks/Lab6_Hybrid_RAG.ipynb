{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Putting It Together \u2014 RAG with Hybrid Retrieval",
    "",
    "**Estimated Time:** 12 minutes",
    "",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up OCI Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import oci\n",
    "\n",
    "genai_client = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "    config=oci.config.from_file(os.getenv(\"OCI_CONFIG_PATH\", \"~/.oci/config\")),\n",
    "    service_endpoint=os.getenv(\"ENDPOINT\")\n",
    ")\n",
    "\n",
    "COMPARTMENT_ID = os.getenv(\"COMPARTMENT_OCID\")\n",
    "\n",
    "def generate_response(prompt, temperature=0.0):\n",
    "    \"\"\"Call OCI Generative AI to generate a response.\"\"\"\n",
    "    chat_detail = oci.generative_ai_inference.models.ChatDetails(\n",
    "        compartment_id=COMPARTMENT_ID,\n",
    "        chat_request=oci.generative_ai_inference.models.GenericChatRequest(\n",
    "            messages=[oci.generative_ai_inference.models.UserMessage(\n",
    "                content=[oci.generative_ai_inference.models.TextContent(text=prompt)]\n",
    "            )],\n",
    "            temperature=temperature,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(\n",
    "            model_id=\"meta.llama-3.2-90b-vision-instruct\"\n",
    "        )\n",
    "    )\n",
    "    response = genai_client.chat(chat_detail)\n",
    "    return response.data.chat_response.choices[0].message.content[0].text\n",
    "\n",
    "print(\"OCI Generative AI client ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Part A \u2014 Simple RAG (Vector Retrieval Only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def retrieve_vector_only(question, top_k=3):\n",
    "    \"\"\"Retrieve chunks using pure vector similarity search.\"\"\"\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT c.chunk_id, c.chunk_text, kb.title,\n",
    "                   ROUND(VECTOR_DISTANCE(c.embedding,\n",
    "                       VECTOR_EMBEDDING(doc_model USING :question),\n",
    "                       COSINE), 4) AS distance\n",
    "            FROM city_knowledge_chunks c\n",
    "            JOIN city_knowledge_base kb ON c.doc_id = kb.doc_id\n",
    "            ORDER BY distance\n",
    "            FETCH APPROXIMATE FIRST :top_k ROWS ONLY\n",
    "        \"\"\", {\"question\": question, \"top_k\": top_k})\n",
    "\n",
    "        results = []\n",
    "        for row in cursor.fetchall():\n",
    "            chunk_text = row[1].read() if hasattr(row[1], 'read') else row[1]\n",
    "            results.append({\n",
    "                \"chunk_id\": row[0], \"text\": chunk_text,\n",
    "                \"source\": row[2], \"score\": row[3]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "print(\"Vector retrieval function ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rag_query(question, retrieval_fn, top_k=3):\n",
    "    \"\"\"Full RAG pipeline using a given retrieval function.\"\"\"\n",
    "    chunks = retrieval_fn(question, top_k=top_k)\n",
    "\n",
    "    context_parts = []\n",
    "    for chunk in chunks:\n",
    "        context_parts.append(f\"[Source: {chunk['source']}]\\n{chunk['text']}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"You are a CityPulse operations assistant. Answer the question using ONLY\n",
    "the provided context below. Be specific and reference relevant details from the\n",
    "source documents. If the context doesn't fully answer the question, say so.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    answer = generate_response(prompt)\n",
    "    return answer, chunks\n",
    "\n",
    "print(\"RAG pipeline ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "QUESTION = \"What is the maintenance history and current status of Substation Gamma in the Harbor district?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART A: RAG WITH VECTOR-ONLY RETRIEVAL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Question: {QUESTION}\\n\")\n",
    "\n",
    "vector_answer, vector_sources = rag_query(QUESTION, retrieve_vector_only)\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(vector_answer)\n",
    "print(\"\\nSOURCES RETRIEVED:\")\n",
    "for s in vector_sources:\n",
    "    print(f\"  \u2022 {s['source']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Part B \u2014 Hybrid RAG"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def retrieve_hybrid(question, top_k=3):\n",
    "    \"\"\"Retrieve chunks using hybrid keyword + vector search.\"\"\"\n",
    "    keywords = \" OR \".join([\n",
    "        word for word in question.split()\n",
    "        if len(word) > 3 and word[0].isupper()\n",
    "    ])\n",
    "    if not keywords:\n",
    "        keywords = \" OR \".join([\n",
    "            word for word in question.split()\n",
    "            if len(word) > 4\n",
    "        ])\n",
    "\n",
    "    return hybrid_search(question, keywords, top_k=top_k,\n",
    "                         keyword_weight=0.4, vector_weight=0.6)\n",
    "\n",
    "print(\"Hybrid retrieval function ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PART B: RAG WITH HYBRID RETRIEVAL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Question: {QUESTION}\\n\")\n",
    "\n",
    "hybrid_answer, hybrid_sources = rag_query(QUESTION, retrieve_hybrid)\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(hybrid_answer)\n",
    "print(\"\\nSOURCES RETRIEVED:\")\n",
    "for s in hybrid_sources:\n",
    "    print(f\"  \u2022 {s['source']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Part C \u2014 Compare Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: VECTOR-ONLY vs. HYBRID RETRIEVAL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nQuestion: {QUESTION}\\n\")\n",
    "\n",
    "print(\"-\" * 35 + \" SOURCES \" + \"-\" * 35)\n",
    "print(\"\\nVector-Only Retrieved:\")\n",
    "for s in vector_sources:\n",
    "    print(f\"  \u2022 {s['source']}\")\n",
    "\n",
    "print(\"\\nHybrid Retrieved:\")\n",
    "for s in hybrid_sources:\n",
    "    print(f\"  \u2022 {s['source']}\")\n",
    "\n",
    "vector_titles = {s['source'] for s in vector_sources}\n",
    "hybrid_titles = {s['source'] for s in hybrid_sources}\n",
    "only_in_hybrid = hybrid_titles - vector_titles\n",
    "only_in_vector = vector_titles - hybrid_titles\n",
    "\n",
    "if only_in_hybrid:\n",
    "    print(f\"\\n  \u2192 Hybrid found that vector missed:\")\n",
    "    for t in only_in_hybrid:\n",
    "        print(f\"    + {t}\")\n",
    "if only_in_vector:\n",
    "    print(f\"\\n  \u2192 Vector found that hybrid missed:\")\n",
    "    for t in only_in_vector:\n",
    "        print(f\"    - {t}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 35 + \" ANSWERS \" + \"-\" * 35)\n",
    "print(\"\\n[VECTOR-ONLY ANSWER]:\")\n",
    "print(vector_answer[:500])\n",
    "print(\"\\n[HYBRID ANSWER]:\")\n",
    "print(hybrid_answer[:500])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Q2 = \"What happened at Harbor Bridge in January 2025 and what were the lessons learned?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Question: {Q2}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "v_answer, v_sources = rag_query(Q2, retrieve_vector_only)\n",
    "h_answer, h_sources = rag_query(Q2, retrieve_hybrid)\n",
    "\n",
    "print(\"\\n[VECTOR-ONLY SOURCES]:\")\n",
    "for s in v_sources: print(f\"  \u2022 {s['source']}\")\n",
    "print(\"\\n[HYBRID SOURCES]:\")\n",
    "for s in h_sources: print(f\"  \u2022 {s['source']}\")\n",
    "\n",
    "print(\"\\n[VECTOR-ONLY ANSWER]:\")\n",
    "print(v_answer[:400])\n",
    "print(\"\\n[HYBRID ANSWER]:\")\n",
    "print(h_answer[:400])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've built a complete RAG pipeline with hybrid retrieval.",
    "",
    "Raw text \u2192 Chunks \u2192 Embeddings \u2192 HNSW Index \u2192 Vector Search \u2192 Hybrid Search \u2192 RAG Pipeline",
    "",
    "The key insight: RAG quality is primarily a **data retrieval** problem. Better retrieval = better answers."
   ]
  }
 ]
}