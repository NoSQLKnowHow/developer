{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Chunk & Embed \u2014 From Text to Vectors",
    "",
    "**Estimated Time:** 6-7 minutes",
    "",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Chunk Documents with VECTOR_CHUNKS",
    "",
    "Embedding models have a maximum input size. Long documents need to be split into smaller **chunks** that fit within that limit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== CHUNKING A LONG DOCUMENT ===\\n\")\n",
    "run_query(\"\"\"\n",
    "    SELECT ROWNUM AS chunk_num,\n",
    "           SUBSTR(C.chunk_text, 1, 80) AS chunk_preview,\n",
    "           LENGTH(C.chunk_text) AS chunk_chars\n",
    "    FROM city_knowledge_base kb,\n",
    "         VECTOR_CHUNKS(kb.content BY WORDS\n",
    "             MAX 200\n",
    "             OVERLAP 40\n",
    "             SPLIT BY SENTENCE) C\n",
    "    WHERE kb.title LIKE 'Harbor Bridge Annual%'\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== CHUNKING A SHORT DOCUMENT ===\\n\")\n",
    "run_query(\"\"\"\n",
    "    SELECT ROWNUM AS chunk_num,\n",
    "           SUBSTR(C.chunk_text, 1, 80) AS chunk_preview,\n",
    "           LENGTH(C.chunk_text) AS chunk_chars\n",
    "    FROM city_knowledge_base kb,\n",
    "         VECTOR_CHUNKS(kb.content BY WORDS\n",
    "             MAX 200\n",
    "             OVERLAP 40\n",
    "             SPLIT BY SENTENCE) C\n",
    "    WHERE kb.title LIKE '%Working Near Energized%'\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== CHUNKS PER DOCUMENT ===\\n\")\n",
    "run_query(\"\"\"\n",
    "    SELECT kb.doc_id,\n",
    "           SUBSTR(kb.title, 1, 50) AS title,\n",
    "           COUNT(*) AS chunk_count\n",
    "    FROM city_knowledge_base kb,\n",
    "         VECTOR_CHUNKS(kb.content BY WORDS\n",
    "             MAX 200\n",
    "             OVERLAP 40\n",
    "             SPLIT BY SENTENCE) C\n",
    "    GROUP BY kb.doc_id, kb.title\n",
    "    ORDER BY chunk_count DESC\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Chunks Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with connection.cursor() as cursor:\n",
    "    # Create the chunks table\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE city_knowledge_chunks (\n",
    "            chunk_id    NUMBER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "            doc_id      NUMBER NOT NULL REFERENCES city_knowledge_base(doc_id),\n",
    "            chunk_text  CLOB,\n",
    "            chunk_pos   NUMBER,\n",
    "            embedding   VECTOR\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Populate with chunks from all documents\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO city_knowledge_chunks (doc_id, chunk_text, chunk_pos)\n",
    "        SELECT kb.doc_id,\n",
    "               C.chunk_text,\n",
    "               C.chunk_offset\n",
    "        FROM city_knowledge_base kb,\n",
    "             VECTOR_CHUNKS(kb.content BY WORDS\n",
    "                 MAX 200\n",
    "                 OVERLAP 40\n",
    "                 SPLIT BY SENTENCE) C\n",
    "    \"\"\")\n",
    "\n",
    "    chunk_count = cursor.rowcount\n",
    "    connection.commit()\n",
    "\n",
    "print(f\"Created city_knowledge_chunks table with {chunk_count} chunks.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== CHUNK TABLE SUMMARY ===\\n\")\n",
    "run_query(\"\"\"\n",
    "    SELECT COUNT(*) AS total_chunks,\n",
    "           ROUND(AVG(LENGTH(chunk_text))) AS avg_chunk_chars,\n",
    "           MIN(LENGTH(chunk_text)) AS min_chunk_chars,\n",
    "           MAX(LENGTH(chunk_text)) AS max_chunk_chars\n",
    "    FROM city_knowledge_chunks\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Vector Embeddings",
    "",
    "Oracle's built-in ONNX embedding model converts each chunk into a vector \u2014 directly inside the database."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE city_knowledge_chunks\n",
    "        SET embedding = VECTOR_EMBEDDING(\n",
    "            doc_model USING chunk_text\n",
    "        )\n",
    "    \"\"\")\n",
    "    updated = cursor.rowcount\n",
    "    connection.commit()\n",
    "\n",
    "print(f\"Generated embeddings for {updated} chunks.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT chunk_id,\n",
    "               SUBSTR(chunk_text, 1, 60) AS preview,\n",
    "               VECTOR_DIMENSION_COUNT(embedding) AS dimensions,\n",
    "               embedding\n",
    "        FROM city_knowledge_chunks\n",
    "        WHERE ROWNUM = 1\n",
    "    \"\"\")\n",
    "    row = cursor.fetchone()\n",
    "\n",
    "print(f\"Chunk ID:   {row[0]}\")\n",
    "print(f\"Preview:    {row[1]}...\")\n",
    "print(f\"Dimensions: {row[2]}\")\n",
    "print(f\"\\nEmbedding (first 10 values):\")\n",
    "vec_str = str(row[3])\n",
    "print(vec_str[:200] + \"...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== EMBEDDING VERIFICATION ===\\n\")\n",
    "run_query(\"\"\"\n",
    "    SELECT COUNT(*) AS total_chunks,\n",
    "           COUNT(embedding) AS with_embedding,\n",
    "           COUNT(*) - COUNT(embedding) AS missing_embedding\n",
    "    FROM city_knowledge_chunks\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your knowledge base is now vectorized and ready for similarity search. **Proceed to Lab 3.**"
   ]
  }
 ]
}