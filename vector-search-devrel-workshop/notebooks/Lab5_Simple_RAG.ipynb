{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Putting It Together \u2014 Simple RAG",
    "",
    "**Estimated Time:** 9 minutes",
    "",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up OCI Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import oci\n",
    "\n",
    "# OCI Generative AI configuration\n",
    "genai_client = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "    config=oci.config.from_file(os.getenv(\"OCI_CONFIG_PATH\", \"~/.oci/config\")),\n",
    "    service_endpoint=os.getenv(\"ENDPOINT\")\n",
    ")\n",
    "\n",
    "COMPARTMENT_ID = os.getenv(\"COMPARTMENT_OCID\")\n",
    "\n",
    "print(\"OCI Generative AI client ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_response(prompt, temperature=0.0):\n",
    "    \"\"\"Call OCI Generative AI to generate a response.\"\"\"\n",
    "    chat_detail = oci.generative_ai_inference.models.ChatDetails(\n",
    "        compartment_id=COMPARTMENT_ID,\n",
    "        chat_request=oci.generative_ai_inference.models.GenericChatRequest(\n",
    "            messages=[oci.generative_ai_inference.models.UserMessage(\n",
    "                content=[oci.generative_ai_inference.models.TextContent(text=prompt)]\n",
    "            )],\n",
    "            temperature=temperature,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        serving_mode=oci.generative_ai_inference.models.OnDemandServingMode(\n",
    "            model_id=\"meta.llama-3.2-90b-vision-instruct\"\n",
    "        )\n",
    "    )\n",
    "    response = genai_client.chat(chat_detail)\n",
    "    return response.data.chat_response.choices[0].message.content[0].text\n",
    "\n",
    "# Quick test\n",
    "test = generate_response(\"Respond with exactly: 'LLM connection verified.'\")\n",
    "print(f\"Test response: {test}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def retrieve_chunks(question, top_k=3):\n",
    "    \"\"\"Retrieve the top-K most relevant chunks using vector search.\"\"\"\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT c.chunk_id,\n",
    "                   c.chunk_text,\n",
    "                   kb.title,\n",
    "                   ROUND(VECTOR_DISTANCE(c.embedding,\n",
    "                       VECTOR_EMBEDDING(doc_model USING :question),\n",
    "                       COSINE), 4) AS distance\n",
    "            FROM city_knowledge_chunks c\n",
    "            JOIN city_knowledge_base kb ON c.doc_id = kb.doc_id\n",
    "            ORDER BY distance\n",
    "            FETCH APPROXIMATE FIRST :top_k ROWS ONLY\n",
    "        \"\"\", {\"question\": question, \"top_k\": top_k})\n",
    "\n",
    "        results = []\n",
    "        for row in cursor.fetchall():\n",
    "            chunk_text = row[1].read() if hasattr(row[1], 'read') else row[1]\n",
    "            results.append({\n",
    "                \"chunk_id\": row[0], \"text\": chunk_text,\n",
    "                \"source\": row[2], \"distance\": row[3]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Test retrieval\n",
    "test_results = retrieve_chunks(\"bridge vibration anomaly response\")\n",
    "print(f\"Retrieved {len(test_results)} chunks:\")\n",
    "for r in test_results:\n",
    "    print(f\"  [{r['distance']}] {r['source'][:60]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rag_query(question, top_k=3):\n",
    "    \"\"\"Full RAG pipeline: retrieve context, build prompt, generate answer.\"\"\"\n",
    "    print(f\"Question: {question}\\n\")\n",
    "\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(\"1. Retrieving relevant context...\")\n",
    "    chunks = retrieve_chunks(question, top_k=top_k)\n",
    "    print(f\"   Found {len(chunks)} relevant chunks.\\n\")\n",
    "\n",
    "    # Step 2: Build context from retrieved chunks\n",
    "    context_parts = []\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        context_parts.append(f\"[Source: {chunk['source']}]\\n{chunk['text']}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # Step 3: Build the prompt\n",
    "    print(\"2. Building prompt with retrieved context...\")\n",
    "    prompt = f\"\"\"You are a CityPulse operations assistant. Answer the question using ONLY\n",
    "the provided context below. Be specific and reference relevant details from the\n",
    "source documents. If the context doesn't fully answer the question, say so.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    # Step 4: Generate response\n",
    "    print(\"3. Generating response with OCI Generative AI...\\n\")\n",
    "    answer = generate_response(prompt)\n",
    "\n",
    "    return answer, chunks\n",
    "\n",
    "print(\"RAG pipeline ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "answer, sources = rag_query(\n",
    "    \"What should I do if a bridge sensor shows elevated vibration readings?\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 70)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SOURCES USED:\")\n",
    "print(\"=\" * 70)\n",
    "for s in sources:\n",
    "    print(f\"  \u2022 {s['source']} (distance: {s['distance']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "answer, sources = rag_query(\n",
    "    \"How do we detect and respond to a possible water main break?\"\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\nSOURCES:\")\n",
    "for s in sources:\n",
    "    print(f\"  \u2022 {s['source']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "answer, sources = rag_query(\n",
    "    \"What infrastructure depends on Substation Gamma and what happens if it fails?\"\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\nSOURCES:\")\n",
    "for s in sources:\n",
    "    print(f\"  \u2022 {s['source']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "answer, sources = rag_query(\n",
    "    \"How do I calibrate a vibration sensor and how often should it be done?\"\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\nSOURCES:\")\n",
    "for s in sources:\n",
    "    print(f\"  \u2022 {s['source']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've built a complete RAG pipeline.",
    "",
    "Raw text \u2192 Chunks \u2192 Embeddings \u2192 HNSW Index \u2192 Vector Search \u2192 RAG Pipeline"
   ]
  }
 ]
}